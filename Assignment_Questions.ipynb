{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Linguistic Pre-processing and Text Representation\n",
    "\n",
    "## Instructions\n",
    "- Answer all questions with detailed explanations\n",
    "- Include code examples where applicable\n",
    "- Provide reasoning for your design choices\n",
    "- Each question requires a comprehensive answer demonstrating understanding of concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Multi-level Linguistic Analysis\n",
    "\n",
    "Consider the sentence: \"The company's CEO didn't respond to our meeting invitation.\"\n",
    "\n",
    "Analyze this sentence from four different linguistic perspectives:\n",
    "- **Syntax**: Identify the grammatical structure and phrase composition\n",
    "- **Semantics**: Explain the meaning and relationships between words\n",
    "- **Morphology**: Break down word formations and their components\n",
    "- **Pragmatics**: Discuss the contextual interpretation and implied meaning\n",
    "\n",
    "**Hint**: Consider how each level provides different insights. For morphology, examine words like \"didn't\" and \"invitation\". For pragmatics, think about what this might imply in a business context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax: The sentence has a subject (‚ÄúThe company‚Äôs CEO‚Äù) and a predicate (‚Äúdidn‚Äôt respond to our meeting invitation‚Äù) forming a negative declarative structure.\n",
    "Semantics: It means the CEO failed to reply to the meeting invitation sent by the speaker‚Äôs group.\n",
    "Morphology: ‚Äúdidn‚Äôt‚Äù = did + not (negation), ‚Äúinvitation‚Äù = invite + -tion (noun-forming suffix).\n",
    "Pragmatics: In a business context, it implies unresponsiveness or possible disinterest from the CEO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax: The sentence has a subject (‚ÄúThe company‚Äôs CEO‚Äù) and a predicate (‚Äúdidn‚Äôt respond to our meeting invitation‚Äù) forming a negative declarative structure.\n",
    "Semantics: It means the CEO failed to reply to the meeting invitation sent by the speaker‚Äôs group.\n",
    "Morphology: ‚Äúdidn‚Äôt‚Äù = did + not (negation), ‚Äúinvitation‚Äù = invite + -tion (noun-forming suffix).\n",
    "Pragmatics: In a business context, it implies unresponsiveness or possible disinterest from the CEO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Pre-processing Pipeline Design\n",
    "\n",
    "You are building a sentiment analysis system for customer reviews from an e-commerce platform. The reviews contain:\n",
    "- Informal language and slang (\"gonna\", \"wanna\", \"u\")\n",
    "- Emojis and special characters\n",
    "- Product codes and prices\n",
    "- Misspellings and typos\n",
    "\n",
    "Design a comprehensive text pre-processing pipeline. For each step (tokenization, normalization, stop-word removal, stemming/lemmatization), explain:\n",
    "1. Why you would include or exclude it\n",
    "2. What specific considerations apply to this use case\n",
    "3. The order of operations and why it matters\n",
    "\n",
    "**Hint**: Consider whether stemming or lemmatization is more appropriate for sentiment analysis. Think about whether removing all special characters is beneficial when emojis carry sentiment information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "\n",
    "Include: Yes, to split text into meaningful units (words, emojis, punctuation).\n",
    "Considerations: Use an emoji-aware tokenizer (like TweetTokenizer or spaCy) \n",
    "Order: First step ‚Äî it enables processing each token (for normalization, spelling correction, etc.).\n",
    "\n",
    "2. Normalization\n",
    "\n",
    "Include: Yes, to standardize text for consistent analysis.\n",
    "Considerations:\n",
    "Convert to lowercase (‚ÄúGreat‚Äù ‚Üí ‚Äúgreat‚Äù).\n",
    "Expand slang (‚Äúgonna‚Äù ‚Üí ‚Äúgoing to‚Äù, ‚Äúu‚Äù ‚Üí ‚Äúyou‚Äù).\n",
    "Remove product codes and prices (they don‚Äôt affect sentiment).\n",
    "Correct common misspellings using a spell-checker (like textblob or symspellpy).\n",
    "Order: After tokenization ‚Äî because normalization acts on tokens.\n",
    "\n",
    "3. Stop-word Removal\n",
    "\n",
    "Include: Yes, but selectively.\n",
    "Considerations: Remove neutral words (like ‚Äúthe‚Äù, ‚Äúis‚Äù), but retain negations (‚Äúnot‚Äù, ‚Äúno‚Äù) since they flip sentiment.\n",
    "Order: After normalization ‚Äî ensures clean, consistent tokens for accurate filtering.\n",
    "\n",
    "4. Stemming / Lemmatization\n",
    "\n",
    "Choice: Lemmatization (better than stemming).\n",
    "Why: Lemmatization keeps words meaningful (‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù), preserving semantic nuance critical for sentiment analysis, while stemming can distort (‚Äúloving‚Äù ‚Üí ‚Äúlove‚Äù is fine, but ‚Äúhappiness‚Äù ‚Üí ‚Äúhappi‚Äù isn‚Äôt).\n",
    "Order: After stop-word removal ‚Äî reduces unnecessary computation on discarded words.\n",
    "\n",
    "5. Optional: Handle Emojis and Punctuation\n",
    "\n",
    "Include: Yes, convert emojis to sentiment words (üòä ‚Üí ‚Äúhappy‚Äù, üò° ‚Üí ‚Äúangry‚Äù).\n",
    "Considerations: Don‚Äôt remove all special characters ‚Äî emojis and exclamation marks (!) often signal emotion intensity.\n",
    "Order: Can occur during or right after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "\n",
    "Include: Yes, to split text into meaningful units (words, emojis, punctuation).\n",
    "Considerations: Use an emoji-aware tokenizer (like TweetTokenizer or spaCy) since they carry sentiment.\n",
    "Order: First step ‚Äî it enables processing each token (for normalization, spelling correction, etc.).\n",
    "\n",
    "2. Normalization\n",
    "Include: Yes, to standardize text for consistent analysis.\n",
    "Considerations:\n",
    "Convert to lowercase (‚ÄúGreat‚Äù ‚Üí ‚Äúgreat‚Äù).\n",
    "Expand slang (‚Äúgonna‚Äù ‚Üí ‚Äúgoing to‚Äù, ‚Äúu‚Äù ‚Üí ‚Äúyou‚Äù).\n",
    "Remove product codes and prices (they don‚Äôt affect sentiment).\n",
    "Correct common misspellings using a spell-checker (like textblob or symspellpy).\n",
    "Order: After tokenization ‚Äî because normalization acts on tokens.\n",
    "\n",
    "3. Stop-word Removal\n",
    "\n",
    "Include: Yes, but selectively.\n",
    "Considerations: Remove neutral words (like ‚Äúthe‚Äù, ‚Äúis‚Äù), but retain negations (‚Äúnot‚Äù, ‚Äúno‚Äù) since they flip sentiment.\n",
    "Order: After normalization ‚Äî ensures clean, consistent tokens for accurate filtering.\n",
    "\n",
    "4. Stemming / Lemmatization\n",
    "\n",
    "Choice: Lemmatization (better than stemming).\n",
    "Why: Lemmatization keeps words meaningful (‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù), preserving semantic nuance critical for sentiment analysis, while stemming can distort (‚Äúloving‚Äù ‚Üí ‚Äúlove‚Äù is fine, but ‚Äúhappiness‚Äù ‚Üí ‚Äúhappi‚Äù isn‚Äôt).\n",
    "Order: After stop-word removal ‚Äî reduces unnecessary computation on discarded words.\n",
    "\n",
    "5. Optional: Handle Emojis and Punctuation\n",
    "\n",
    "Include: Yes, convert emojis to sentiment words (üòä ‚Üí ‚Äúhappy‚Äù, üò° ‚Üí ‚Äúangry‚Äù).\n",
    "Considerations: Don‚Äôt remove all special characters ‚Äî emojis and exclamation marks (!) often signal emotion intensity.\n",
    "Order: Can occur during or right after normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Stemming vs Lemmatization Trade-offs\n",
    "\n",
    "Consider these sentences:\n",
    "1. \"The meeting was well organized and the organizers did a great job.\"\n",
    "2. \"She is better at organizing than her predecessor was.\"\n",
    "\n",
    "Apply both stemming (Porter Stemmer) and lemmatization to these sentences. Then:\n",
    "- Compare the outputs and explain the differences\n",
    "- Discuss scenarios where stemming would be preferred over lemmatization and vice versa\n",
    "- Analyze the impact on: search engines, text classification, and information retrieval systems\n",
    "\n",
    "**Hint**: Consider computational cost, accuracy, and preservation of meaning. Words like \"better\", \"organizing\", and \"was\" behave differently under stemming vs lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 1:\n",
    "\n",
    "‚ÄúThe meeting was well organized and the organizers did a great job.‚Äù\n",
    "Stemming: ‚Äúthe meet wa well organ and the organ did a great job‚Äù\n",
    "Lemmatization: ‚Äúthe meet be well organize and the organizer do a great job‚Äù\n",
    "\n",
    "Sentence 2:\n",
    "\n",
    "‚ÄúShe is better at organizing than her predecessor was.‚Äù\n",
    "Stemming: ‚Äúshe is better at organ than her predecess wa‚Äù\n",
    "Lemmatization: ‚Äúshe be good at organize than her predecessor be‚Äù\n",
    "\n",
    "\n",
    "-- Stemming simply chops off endings using fixed rules and may produce non-words (like ‚Äúorgan‚Äù or ‚Äúwa‚Äù). where as  Lemmatization uses a dictionary and grammar rules to return valid base forms (like ‚Äúorganize‚Äù or ‚Äúbe‚Äù).\n",
    "\n",
    "---Using  stemming when processing large text collections quickly like search engines while  Using  lemmatization when meaning matters (like sentiment analysis or text classification).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 1:\n",
    "\n",
    "‚ÄúThe meeting was well organized and the organizers did a great job.‚Äù\n",
    "Stemming: ‚Äúthe meet wa well organ and the organ did a great job‚Äù\n",
    "Lemmatization: ‚Äúthe meet be well organize and the organizer do a great job‚Äù\n",
    "\n",
    "Sentence 2:\n",
    "\n",
    "‚ÄúShe is better at organizing than her predecessor was.‚Äù\n",
    "Stemming: ‚Äúshe is better at organ than her predecess wa‚Äù\n",
    "Lematization: ‚Äúshe be good at organize than her predecessor be‚Äù\n",
    "\n",
    "Differences between Stemming and Lemmatization\n",
    "Stemming simply chops off endings using fixed rules and may produce non-words (like ‚Äúorgan‚Äù or ‚Äúwa‚Äù).\n",
    "Lematization uses a dictionary and grammar rules to return valid base forms (like ‚Äúorganize‚Äù or ‚Äúbe‚Äù).\n",
    "Lemmatization handles irregular words correctly (‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù, ‚Äúwas‚Äù ‚Üí ‚Äúbe‚Äù), while stemming does not.\n",
    "Stemming is much faster but less accurate; lemmatization is slower but more meaningful.\n",
    "\n",
    "When to Use Each\n",
    "Use stemming when processing large text collections quickly (like search engines).\n",
    "Use lemmatization when meaning matters (like sentiment analysis or text classification).\n",
    "Stemming focuses on speed; lemmatization focuses on linguistic accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: POS Tagging for Ambiguity Resolution\n",
    "\n",
    "Examine these ambiguous sentences:\n",
    "1. \"The duck is ready to eat.\"\n",
    "2. \"They can fish.\"\n",
    "3. \"Time flies like an arrow.\"\n",
    "\n",
    "Explain:\n",
    "- How POS tagging helps resolve these ambiguities\n",
    "- The difference between rule-based and probabilistic POS tagging approaches\n",
    "- Which approach would perform better for each sentence and why\n",
    "- Limitations of both approaches\n",
    "\n",
    "**Hint**: Consider how context and word order influence tagging. Think about the Hidden Markov Model approach for probabilistic tagging vs pattern-matching rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--POS tagging assigns parts of speech (noun, verb, adjective, etc.) to each word.Ambiguities occur when a word can be multiple POS types depending on context.\n",
    "Example: ‚Äúduck‚Äù can be a noun (the bird) or a verb (to lower your head).\n",
    "\n",
    "Rule_based tags\n",
    "Uses hand-crafted rules based on word patterns, suffixes, capitalization, and neighboring words.\n",
    "Example: If a word follows ‚Äúthe‚Äù, it‚Äôs likely a noun.\n",
    "\n",
    "Prob Tags\n",
    "Uses statistical models (like Hidden Markov Models) trained on large corpora. Assigns tags based on likelihoods of sequences of POS tags.\n",
    "\n",
    "sent 1:Best approach: Probabilistic ‚Äî can use context: ‚ÄúThe [noun] is ready to eat‚Äù is more likely than ‚ÄúThe [verb] is ready‚Ä¶‚Äù.\n",
    "Sent 2:Best approach: Probabilistic ‚Äî context ‚ÄúThey [modal] [verb]‚Äù resolves it as ‚ÄúThey can [verb] fish‚Äù (ability), which rule-based may misinterpret.\n",
    "sent 3:Best approach: Probabilistic ‚Äî considers sequence probabilities to tag ‚ÄúTime [noun] flies [verb] like [prep] an arrow‚Äù.\n",
    "\n",
    "\n",
    "----\n",
    "4. Limitations\n",
    "\n",
    "Rule-based:\n",
    "Cannot handle unusual or idiomatic sentences easily. Requires extensive manual rules.\n",
    "\n",
    "Probabilistic:\n",
    "Depends on quality and size of training corpus.May misclassify rare or creative uses of language.Cannot fully understand semantics, only probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How POS Tagging Resolves Ambiguities\n",
    "POS tagging assigns parts of speech (noun, verb, adjective, etc.) to each word.\n",
    "Ambiguities occur when a word can be multiple POS types depending on context.\n",
    "Example: ‚Äúduck‚Äù can be a noun (the bird) or a verb (to lower your head).\n",
    "Correct POS tags help disambiguate meaning in a sentence, which is crucial for parsing, semantic analysis, and NLP tasks.\n",
    "\n",
    "2. Rule-based vs Probabilistic POS Tagging\n",
    "\n",
    "Rule-based POS Tagging:\n",
    "Uses hand-crafted rules based on word patterns, suffixes, capitalization, and neighboring words.\n",
    "Exmple: If a word follows ‚Äúthe‚Äù, it‚Äôs likely a noun.\n",
    "Pros: Transparent, interpretable.\n",
    "Cons: Hard to cover all cases, fails with unseen patterns or idiomatic expressions.\n",
    "\n",
    "Probabilistic POS Tagging:\n",
    "Uses statistical models (like Hidden Markov Models) trained on large corpora.\n",
    "Assigns tags based on likelihoods of sequences of POS tags.\n",
    "Pros: Handles context better, adapts to new words via probabilities.\n",
    "Cons: Requires annotated corpora, may make mistakes if the context is unusual.\n",
    "\n",
    "3. Performance for Each Sentence\n",
    "\n",
    "Sentence 1: ‚ÄúThe duck is ready to eat.‚Äù\n",
    "Ambiguity: ‚Äúduck‚Äù = noun (bird) or verb (action).\n",
    "Best approach: Probabilistic ‚Äî can use context: ‚ÄúThe [noun] is ready to eat‚Äù is more likely than ‚ÄúThe [verb] is ready‚Ä¶‚Äù.\n",
    "Sentence 2: ‚ÄúThey can fish.‚Äù\n",
    "Ambiguity: ‚Äúcan‚Äù = modal verb or container, ‚Äúfish‚Äù = noun or verb.\n",
    "Best approach: Probabilistic ‚Äî context ‚ÄúThey [modal] [verb]‚Äù resolves it as ‚ÄúThey can [verb] fish‚Äù (ability), which rule-based may misinterpret.\n",
    "\n",
    "Sentence 3: ‚ÄúTime flies like an arrow.‚Äù\n",
    "Ambiguity: ‚Äúflies‚Äù = verb or plural noun, ‚Äúlike‚Äù = preposition or verb.\n",
    "Best approach: Probabilistic ‚Äî considers sequence probabilities to tag ‚ÄúTime [noun] flies [verb] like [prep] an arrow‚Äù\n",
    "Rule-based may fail due to idiomatic structure.\n",
    "\n",
    "4. Limitations\n",
    "\n",
    "Rule-based:\n",
    "Cannot handle unusual or idiomatic sentences easily.\n",
    "Requires extensive manual rules.\n",
    "Probabilistic:\n",
    "Depends on quality and size of training corpus.\n",
    "May misclassify rare or creative uses of language.\n",
    "Cannot fully understand semantics, only probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Named Entity Recognition System Design\n",
    "\n",
    "You need to build an NER system for extracting information from medical reports. The text contains:\n",
    "- Disease names (\"Type 2 Diabetes\", \"COVID-19\")\n",
    "- Medication names (\"Metformin\", \"Ibuprofen 200mg\")\n",
    "- Dosages and measurements\n",
    "- Doctor and patient names\n",
    "- Hospital names and dates\n",
    "\n",
    "Compare dictionary-based and CRF-based NER methods for this application:\n",
    "- Advantages and disadvantages of each approach\n",
    "- How would you handle new drug names not in the dictionary?\n",
    "- What features would you use in a CRF model?\n",
    "- How would you combine both approaches for optimal results?\n",
    "\n",
    "**Hint**: Consider that medical terminology is specialized but relatively standardized. Think about feature engineering for CRF models (capitalization, word shape, surrounding words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dictionary-based NER\n",
    "\n",
    "Advantages:\n",
    "Simple and fast to implement. Works well for standardized medical terms (common diseases, drug names).High precision when the dictionary contains the term.\n",
    "\n",
    "Disadvantages:\n",
    "Cannot detect new or misspelled entities.Sensitive to variations (e.g., ‚ÄúMetformin 500mg‚Äù vs ‚ÄúMetformin‚Äù).Limited context understanding (e.g., ‚Äúdiabetes‚Äù vs ‚Äúprediabetes‚Äù).\n",
    "\n",
    "Handling new drug names:\n",
    "Update the dictionary regularly from drug databases (RxNorm, DrugBank).Use fuzzy matching to capture typos or minor variations.Combine with pattern-based rules for dosages (e.g., ‚Äú\\d+mg‚Äù).\n",
    "\n",
    "2. CRF-based NER\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Context-aware ‚Äî considers surrounding words to identify entities.Can recognize unseen entities if patterns/features match training data.Handles complex entities like ‚ÄúCOVID-19 vaccine 2nd dose‚Äù.\n",
    "\n",
    "Disadvantages:\n",
    "Requires labeled training data, which is costly for medical text.Slower to train and process than dictionary lookup.Performance depends on feature engineering.\n",
    "\n",
    "Features for CRF model:\n",
    "\n",
    "Lexical features: word itself, lowercase form, prefixes/suffixes.\n",
    "Orthographic features: capitalization, digits, special characters (e.g., hyphens in ‚ÄúCOVID-19‚Äù).\n",
    "Context features: neighboring words (window of ¬±2 words).\n",
    "Part-of-speech tags: verbs, nouns, numbers.\n",
    "Word shape: patterns like ‚ÄúXx‚Äù, ‚ÄúXX‚Äù, ‚Äúdd‚Äù for numeric dosages.\n",
    "Dictionary features: flag if a word exists in medical dictionaries.\n",
    "\n",
    "3. Combining Both Approaches\n",
    "\n",
    "Strategy for optimal results:\n",
    "Use dictionary-based lookup first to capture known standardized entities (high precision).\n",
    "\n",
    "Use CRF-based tagging for:\n",
    "Unseen drug names or variationEntities requiring context to disambiguate (e.g., ‚ÄúType 2 Diabetes‚Äù vs ‚Äútype‚Äù).\n",
    "\n",
    "Post-processing\n",
    "Resolve conflicts (if CRF and dictionary disagree).Normalize entities (e.g., unify ‚ÄúMetformin 500mg‚Äù ‚Üí ‚ÄúMetformin‚Äù).\n",
    "\n",
    "Benefits of combination:\n",
    "High precision from dictionary-based approach.\n",
    "High recall from CRF model handling unseen or context-dependent entities.\n",
    "Handles misspellings, dosage patterns, and complex phrases effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Dictionary-based NER\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and fast to implement.Works well for standardized medical terms (common diseases, drug names)High precision when the dictionary contains the term.\n",
    "\n",
    "Disadvantages:\n",
    "Cannot detect new or misspelled entities.Sensitive to variations (e.g., ‚ÄúMetformin 500mg‚Äù vs ‚ÄúMetformin‚Äù).\n",
    "Limited context understanding (e.g., ‚Äúdiabetes‚Äù vs ‚Äúprediabetes‚Äù)Handling new drug names:\n",
    "Update the dictionary regularly from drug databases (RxNorm, DrugBank).\n",
    "Use fuzzy matching to capture typos or minor variations.\n",
    "Combine with pattern-based rules for dosages (e.g., ‚Äú\\d+mg‚Äù).\n",
    "\n",
    "2. CRF-based NER\n",
    "\n",
    "Advantages:\n",
    "Context-aware ‚Äî considers surrounding words to identify entities.\n",
    "Can recognize unseen entities if patterns/features match training data.\n",
    "Handles complex entities like ‚ÄúCOVID-19 vaccine 2nd dose‚Äù.\n",
    "\n",
    "Disadvantages:\n",
    "Requires labeled training data, which is costly for medical text.\n",
    "Slower to train and process than dictionary lookup.\n",
    "Performance depends on feature engineering.\n",
    "\n",
    "Features for CRF model:\n",
    "Lexical features: word itself, lowercase form, prefixes/suffixes.\n",
    "Orthographic features: capitalization, digits, special characters (e.g., hyphens in ‚ÄúCOVID-19‚Äù).\n",
    "Cntext features: neighboring words (window of ¬±2 words).\n",
    "Part-of-speech tags: verbs, nouns, numbers.\n",
    "Word shape: patterns like ‚ÄúXx‚Äù, ‚ÄúXX‚Äù, ‚Äúdd‚Äù for numeric dosages.\n",
    "Dictionary features: flag if a word exists in medical dictionaries.\n",
    "\n",
    "3. Combining Both Approaches\n",
    "\n",
    "Strategy for optimal results:\n",
    "Use dictionary-based lookup first to capture known standardized entities (high precision).\n",
    "Use CRF-based tagging for:\n",
    "Unseen drug names or variations\n",
    "Entities requiring context to disambiguate (e.g., ‚ÄúType 2 Diabetes‚Äù vs ‚Äútype‚Äù).\n",
    "\n",
    "Post-processing:\n",
    "\n",
    "Resolve conflicts (if CRF and dictionary disagree)Normalize entities (e.g., unify ‚ÄúMetformin 500mg‚Äù ‚Üí ‚ÄúMetformin‚Äù).\n",
    "Benefits of combination:\n",
    "High precision from dictionary-based approach.\n",
    "High recall from CRF model handling unseen or context-dependent entities.\n",
    "Handles misspellings, dosage patterns, and complex phrases effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: N-gram Language Models and Perplexity\n",
    "\n",
    "Given a small corpus:\n",
    "```\n",
    "\"I love machine learning\"\n",
    "\"I love deep learning\"\n",
    "\"Machine learning is fascinating\"\n",
    "\"Deep learning is powerful\"\n",
    "```\n",
    "\n",
    "a) Build a bigram language model and calculate probabilities for:\n",
    "   - \"I love natural learning\"\n",
    "   - \"Machine learning is powerful\"\n",
    "\n",
    "b) Explain the zero-probability problem and demonstrate:\n",
    "   - How Laplace smoothing addresses it\n",
    "   - The concept of backoff strategies\n",
    "   - How to calculate and interpret perplexity\n",
    "\n",
    "c) Discuss why lower perplexity indicates a better language model.\n",
    "\n",
    "**Hint**: For unseen bigrams like \"natural learning\", consider what probability would be assigned without smoothing. Calculate perplexity as a measure of how \"surprised\" the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Bigram Model Calculation:\n",
    "A bigram model estimates probabilities like P(word‚ÇÇ | word‚ÇÅ). For example, P(love | I) = 1 and P(learning | machine) = 1 based on the corpus. The phrase ‚ÄúI love natural learning‚Äù has zero probability since ‚Äúnatural‚Äù never follows ‚Äúlove,‚Äù while ‚ÄúMachine learning is powerful‚Äù has a valid probability product > 0.\n",
    "\n",
    "b) Zero-Probability & Solutions:\n",
    "The zero-probability problem occurs when unseen bigrams (like ‚Äúnatural learning‚Äù) make the entire sentence probability zero. Laplace smoothing adds 1 to all counts, ensuring no bigram has zero probability. Backoff models assign probability from smaller n-grams (like unigrams) when higher-order n-grams are missing, making the model more robust.\n",
    "\n",
    "c) Perplexity & Model Quality:\n",
    "Perplexity measures how well a model predicts test data ‚Äî lower perplexity means the model is less ‚Äúsurprised‚Äù by real sentences. It indicates better generalization and smoother probability distribution. In essence, a lower perplexity model predicts natural language more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Bigram Model Calculation:\n",
    "A bigram model estimates probabilities like P(word‚ÇÇ | word‚ÇÅ). For example, P(love | I) = 1 and P(learning | machine) = 1 based on the corpus. The phrase ‚ÄúI love natural learning‚Äù has zero probability since ‚Äúnatural‚Äù never follows ‚Äúlove,‚Äù while ‚ÄúMachine learning is powerful‚Äù has a valid probability product > 0.\n",
    "\n",
    "b) Zero-Probability & Solutions:\n",
    "The zero-probability problem occurs when unseen bigrams (like ‚Äúnatural learning‚Äù) make the entire sentence probability zero. Laplace smoothing adds 1 to all counts, ensuring no bigram has zero probability. Backoff models assign probability from smaller n-grams (like unigrams) when higher-order n-grams are missing, making the model more robust.\n",
    "\n",
    "c) Perplexity & Model Quality:\n",
    "Perplexity measures how well a model predicts test data ‚Äî lower perplexity means the model is less ‚Äúsurprised‚Äù by real sentences. It indicates better generalization and smoother probability distribution. In essence, a lower perplexity model predicts natural language more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Bag-of-Words vs TF-IDF Analysis\n",
    "\n",
    "Consider three documents:\n",
    "- Doc1: \"Machine learning is a subset of artificial intelligence\"\n",
    "- Doc2: \"Deep learning is a subset of machine learning\"\n",
    "- Doc3: \"Artificial intelligence and machine learning are transforming industries\"\n",
    "\n",
    "a) Construct the BoW representation and TF-IDF vectors for all documents\n",
    "\n",
    "b) Calculate cosine similarity between documents using both representations\n",
    "\n",
    "c) Explain:\n",
    "   - Why the similarity scores differ between BoW and TF-IDF\n",
    "   - Which representation better captures document similarity for:\n",
    "     - Information retrieval\n",
    "     - Document clustering\n",
    "     - Topic modeling\n",
    "   - Limitations of both approaches\n",
    "\n",
    "**Hint**: Consider how TF-IDF downweights common terms like \"is\" and \"a\". Think about what information is lost (word order, context, semantics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Q7: Bag-of-Words vs TF-IDF Analysis\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Artificial intelligence and machine learning are transforming industries\"\n",
    "]\n",
    "\n",
    "# --- (a) BoW and TF-IDF Representations ---\n",
    "\n",
    "# Create Bag-of-Words model\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(docs)\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create TF-IDF model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display both representations\n",
    "print(\"=== Bag-of-Words Representation ===\")\n",
    "print(bow_df)\n",
    "print(\"\\n=== TF-IDF Representation ===\")\n",
    "print(tfidf_df.round(4))\n",
    "\n",
    "# --- (b) Cosine Similarity Calculations ---\n",
    "\n",
    "# Compute cosine similarity for BoW\n",
    "bow_cosine_sim = cosine_similarity(bow_matrix)\n",
    "print(\"\\n=== Cosine Similarity (BoW) ===\")\n",
    "print(pd.DataFrame(bow_cosine_sim, \n",
    "                   index=['Doc1', 'Doc2', 'Doc3'], \n",
    "                   columns=['Doc1', 'Doc2', 'Doc3']).round(3))\n",
    "\n",
    "# Compute cosine similarity for TF-IDF\n",
    "tfidf_cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "print(\"\\n=== Cosine Similarity (TF-IDF) ===\")\n",
    "print(pd.DataFrame(tfidf_cosine_sim, \n",
    "                   index=['Doc1', 'Doc2', 'Doc3'], \n",
    "                   columns=['Doc1', 'Doc2', 'Doc3']).round(3))\n",
    "\n",
    "# --- (c) Observations ---\n",
    "print(\"\\n--- Observations ---\")\n",
    "print(\"‚Ä¢ BoW gives higher similarity scores for all pairs since frequent terms dominate (like 'machine' and 'learning').\")\n",
    "print(\"‚Ä¢ TF-IDF downweights common words and emphasizes unique ones (like 'deep', 'transforming'), reducing overlap.\")\n",
    "print(\"‚Ä¢ Thus, TF-IDF is better for information retrieval and clustering, while BoW works well for topic modeling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Word2Vec Architectures Deep Dive\n",
    "\n",
    "Explain the Word2Vec model by addressing:\n",
    "\n",
    "a) **CBOW (Continuous Bag of Words)**:\n",
    "   - Architecture and training objective\n",
    "   - How context words predict the target word\n",
    "   - Best use cases\n",
    "\n",
    "b) **Skip-gram**:\n",
    "   - Architecture and training objective\n",
    "   - How target word predicts context words\n",
    "   - Best use cases\n",
    "\n",
    "c) For the sentence \"The quick brown fox jumps over the lazy dog\" (window size = 2):\n",
    "   - Show training examples for both CBOW and Skip-gram when target word is \"fox\"\n",
    "   - Explain which architecture works better for:\n",
    "     - Small datasets\n",
    "     - Rare words\n",
    "     - Frequent words\n",
    "\n",
    "**Hint**: CBOW is faster and works well with frequent words, while Skip-gram is better for rare words and smaller datasets. Consider the number of training instances generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) CBOW (Continuous Bag of Words)\n",
    "\n",
    "Architecture: Uses surrounding context words within a window to predict the target word.\n",
    "Objective: Maximize P(target | context).\n",
    "Use case: Fast and efficient for large datasets with frequent words.\n",
    "\n",
    "b) Skip-gram\n",
    "\n",
    "Architecture: Uses the target word to predict its context words.\n",
    "Objective: Maximize P(context | target).\n",
    "Use case: Better for small datasets and learning rare word representations.\n",
    "\n",
    "c) Example ‚Äî ‚ÄúThe quick brown fox jumps over the lazy dog‚Äù (window = 2, target = ‚Äúfox‚Äù)\n",
    "\n",
    "CBOW training pair:\n",
    "Context = [‚Äúquick‚Äù, ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù] ‚Üí Target = ‚Äúfox‚Äù.\n",
    "Skip-gram training pairs:\n",
    "Target = ‚Äúfox‚Äù ‚Üí Context words = (‚Äúquick‚Äù, ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù).\n",
    "Performance:\n",
    "Small datasets: Skip-gram performs better.\n",
    "Rare words: Skip-gram learns them more effectively.\n",
    "Frequent words: CBOW trains faster and performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) CBOW (Continuous Bag of Words)\n",
    "\n",
    "\n",
    "Architecture: Uses surrounding context words within a window to predict the target word.\n",
    "Objective: Maximize P(target | context).\n",
    "Use case: Fast and efficient for large datasets with frequent words.\n",
    "\n",
    "\n",
    "\n",
    "b) Skip-gram\n",
    "Architecture: Uses the target word to predict its context words.\n",
    "Objective: Maximize P(context | target).\n",
    "Use case: Better for small datasets and learning rare word representations.\n",
    "\n",
    "\n",
    "\n",
    "c) Example ‚Äî ‚ÄúThe quick brown fox jumps over the lazy dog‚Äù (window = 2, target = ‚Äúfox‚Äù)\n",
    "\n",
    "CBOW training pair:\n",
    "Context = [‚Äúquick‚Äù, ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù] ‚Üí Target = ‚Äúfox‚Äù.\n",
    "Skip-gram training pairs:\n",
    "Target = ‚Äúfox‚Äù ‚Üí Context words = (‚Äúquick‚Äù, ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù).\n",
    "\n",
    "Performance:\n",
    "Small datasets: Skip-gram performs better.\n",
    "Rare words: Skip-gram learns them more effectively.\n",
    "Frequent words: CBOW trains faster and performs better.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: GloVe vs FastText Comparison\n",
    "\n",
    "Compare and contrast GloVe and FastText embedding techniques:\n",
    "\n",
    "a) **Training methodology**:\n",
    "   - How does GloVe use global co-occurrence statistics?\n",
    "   - How does FastText incorporate subword information?\n",
    "\n",
    "b) **Handling Out-of-Vocabulary (OOV) words**:\n",
    "   - Given the trained words: \"playing\", \"player\", \"played\"\n",
    "   - How would each model handle the unseen word \"gameplay\"?\n",
    "   - Which model is more suitable for morphologically rich languages (e.g., German, Turkish)?\n",
    "\n",
    "c) **Practical considerations**:\n",
    "   - Training time and computational requirements\n",
    "   - Model size and memory footprint\n",
    "   - Performance on rare and misspelled words\n",
    "\n",
    "**Hint**: FastText breaks words into character n-grams (e.g., \"playing\" ‚Üí \"<pl\", \"pla\", \"lay\", \"ayi\", \"yin\", \"ing\", \"ng>\"). GloVe uses matrix factorization on co-occurrence counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Training Methodology\n",
    "\n",
    "GloVe: Uses global co-occurrence statistics ‚Äî builds a word‚Äìcontext matrix and learns embeddings by factorizing it so similar words appear close in vector space.\n",
    "\n",
    "FastText: Extends Word2Vec by representing each word as a bag of character n-grams, learning subword embeddings that capture morphology.\n",
    "\n",
    "b) Handling OOV (Out-of-Vocabulary) Words\n",
    "\n",
    "Example: Trained on ‚Äúplaying‚Äù, ‚Äúplayer‚Äù, ‚Äúplayed‚Äù.\n",
    "\n",
    "GloVe: Cannot represent unseen word ‚Äúgameplay‚Äù ‚Äî it‚Äôs truly OOV.\n",
    "\n",
    "FastText: Generates a vector by combining n-grams (e.g., ‚Äúgam‚Äù, ‚Äúame‚Äù, ‚Äúmep‚Äù, ‚Äúpla‚Äù, ‚Äúlay‚Äù) ‚Üí can infer a meaningful embedding.\n",
    "\n",
    "Morphologically rich languages: FastText performs better since it handles prefixes, suffixes, and inflections effectively.\n",
    "\n",
    "c) Practical Considerations\n",
    "\n",
    "Training time: GloVe is faster (matrix factorization) but less flexible; FastText is slower due to subword computations.\n",
    "\n",
    "Model size: FastText models are larger since they store n-gram embeddings; GloVe is more compact.\n",
    "\n",
    "Rare/misspelled words: FastText handles them better by composing subword features; GloVe fails for unseen or rare forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Training Methodology\n",
    "\n",
    "GloVe: Uses global co-occurrence statistics ‚Äî builds a word‚Äìcontext matrix and learns embeddings by factorizing it so similar words appear close in vector space.\n",
    "FastText: Extends Word2Vec by representing each word as a bag of character n-grams, learning subword embeddings that capture morphology.\n",
    "\n",
    "b) Handling OOV (Out-of-Vocabulary) Words\n",
    "\n",
    "Example: Trained on ‚Äúplaying‚Äù, ‚Äúplayer‚Äù, ‚Äúplayed‚Äù.\n",
    "GloVe: Cannot represent unseen word ‚Äúgameplay‚Äù ‚Äî it‚Äôs truly OOV.\n",
    "FastText: Generates a vector by combining n-grams (e.g., ‚Äúgam‚Äù, ‚Äúame‚Äù, ‚Äúmep‚Äù, ‚Äúpla‚Äù, ‚Äúlay‚Äù) ‚Üí can infer a meaningful embedding.\n",
    "Morphologically rich languages: FastText performs better since it handles prefixes, suffixes, and inflections effectively.\n",
    "\n",
    "c) Practical Considerations\n",
    "\n",
    "Training time: GloVe is faster (matrix factorization) but less flexible; FastText is slower due to subword computations.\n",
    "Model size: FastText models are larger since they store n-gram embeddings; GloVe is more compact.\n",
    "Rare/misspelled words: FastText handles them better by composing subword features; GloVe fails for unseen or rare forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Classical vs Distributed Representations - Application Perspective\n",
    "\n",
    "You are tasked with building three different NLP applications:\n",
    "\n",
    "1. **Legal document search engine** (searching through contracts and legal texts)\n",
    "2. **Chatbot intent classification** (understanding user queries)\n",
    "3. **Academic paper recommendation system** (suggesting related research papers)\n",
    "\n",
    "For each application:\n",
    "\n",
    "a) Decide whether to use classical representations (BoW/TF-IDF) or distributed representations (Word2Vec/GloVe/FastText)\n",
    "\n",
    "b) Justify your choice by considering:\n",
    "   - Semantic similarity requirements\n",
    "   - Vocabulary size and domain specificity\n",
    "   - Training data availability\n",
    "   - Computational constraints\n",
    "   - Interpretability needs\n",
    "\n",
    "c) Discuss hybrid approaches: Could combining both representation types improve performance? How?\n",
    "\n",
    "**Hint**: Legal documents might require exact term matching, while chatbots benefit from semantic understanding. Consider that classical methods are sparse and interpretable, while distributed representations are dense and capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (use markdown or code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Legal Document Search Engine\n",
    "\n",
    "a) Representation: Classical (TF-IDF)\n",
    "b) Justification:\n",
    "Legal documents rely on exact term matching and interpretability. TF-IDF is ideal because it highlights important legal terms, handles domain-specific vocabulary, and doesn‚Äôt need massive training data. It‚Äôs computationally efficient and easy to explain to users.\n",
    "c) Hybrid Approach:\n",
    "Combining TF-IDF with Word2Vec embeddings can help retrieve documents that are semantically related even if they use different legal terms (e.g., ‚Äúagreement‚Äù vs. ‚Äúcontract‚Äù).\n",
    "\n",
    "a) Representation: Distributed (Word2Vec or FastText)\n",
    "b) Justification:\n",
    "Chatbots must understand semantic similarity and handle diverse phrasing (e.g., ‚Äúbook flight‚Äù ‚âà ‚Äúreserve a ticket‚Äù). Distributed embeddings capture meaning beyond word surface forms, improving generalization and intent recognition accuracy.\n",
    "c) Hybrid Approach:\n",
    "Combining embeddings with TF-IDF features can balance interpretability and context, improving predictions for rare intents or domain-specific terms.\n",
    "\n",
    "Academic Paper Recommendation System\n",
    "\n",
    "a) Representation: Distributed (Doc2Vec or Sentence Embeddings)\n",
    "b) Justification:\n",
    "Recommendations require understanding semantic relationships between research papers (e.g., ‚Äúneural networks‚Äù vs. ‚Äúdeep learning‚Äù). Distributed embeddings capture contextual meaning, enabling better similarity search across large vocabularies.\n",
    "c) Hybrid Approach:\n",
    "A hybrid model combining TF-IDF (for keyword precision) and embeddings (for topic similarity) can boost recommendation quality, ensuring both relevance and coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines\n",
    "\n",
    "- Complete all questions in this notebook\n",
    "- Include code implementations where applicable (using NLTK, spaCy, scikit-learn, or gensim)\n",
    "- Provide clear explanations and reasoning\n",
    "- Add visualizations if they help explain your answers\n",
    "- Ensure your code is properly commented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
